{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Descargar los recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTE ES EL SEGUNDO, QUE ES IDENTICO.\n",
    "\n",
    "\n",
    "ruta_archivo = r'data\\filtered_output\\google-strict-filtered-reviews.jsonl' # RUTA DEL ARCHIVO QUE QUIEREN FILTRAR\n",
    "\n",
    "\n",
    "# Conjunto de palabras vacías (stop words)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Abrir el archivo JSONL\n",
    "with jsonlines.open(ruta_archivo) as archivo_jsonl:\n",
    "    # Recorrer cada línea en el archivo\n",
    "    for linea in archivo_jsonl:\n",
    "        if 'text' in linea and linea['text'] is not None:\n",
    "            # Obtener el texto y rating de la línea\n",
    "            texto = linea['text']\n",
    "            rating = linea['rating']\n",
    "            \n",
    "            # Verificar si el rating es menor o igual a 2\n",
    "            if rating <= 2:\n",
    "                # Preprocesamiento del texto\n",
    "                # Removal of invalid characters\n",
    "                texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "\n",
    "                # Tokenization\n",
    "                tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "                # Remove stopwords\n",
    "                \n",
    "                tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "                # Join tokens back into a single string\n",
    "                preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "                # Tokenizar el texto preprocesado en oraciones y palabras\n",
    "                oraciones = sent_tokenize(preprocessed_text)\n",
    "                palabras = [word.lower() for oracion in oraciones for word in word_tokenize(oracion)]\n",
    "\n",
    "                # Etiquetar las palabras con su tipo (POS tagging)\n",
    "                palabras_etiquetadas = pos_tag(palabras)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Imprimir palabras sin limitar la cantidad\n",
    "for tipo, palabras in palabras_por_tipo.items():\n",
    "    # Contar la frecuencia de las palabras\n",
    "    contador_palabras = Counter(palabras)\n",
    "    # Ordenar las palabras por frecuencia\n",
    "    palabras_ordenadas = sorted(contador_palabras.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"{tipo}:\")\n",
    "    for palabra, frecuencia in palabras_ordenadas:\n",
    "        print(f\"{palabra}: {frecuencia}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que en base al puntaje del bar, tome las reviews de 3 o menos, guarde palabras claves basadas en la lista de palabras qe esta separado en las 4 categorias y te devuelva las categorias con mas coincidencias.\n",
    "esto podria hacerse con kmeans, con logical regresion, con vectorizacion igual a la del modelo del pi 1, o cualquier otra cosa.\n",
    "pero si o si hay que hacer un filtro completo con las palabras correctas para el resultado esperado.\n",
    "y eso si es un modelo de ml en teoria, pq sabemos que hay 4 categorias, pero no sabemos cual de esas van a ser las que van a resultar del analisis de las reviews. el problema que tengo con esto es que si son bares con pocas reviews y muchas reviews nulas, nos vale verga.\n",
    "asi que como filtro inicial tenemos que filtrar las reviews que tengan text null, y a partir de ahi, volver a filtrar los gmap_id que tengan menos de x cantidad de reviews. \n",
    "de ahi podemos hacer el filtro inverso, agarrar el gmap_id de reviews y sacar todos los bares que no esten en los reviews.\n",
    "eso nos dejaria muchos menos datos para trabajar en general.\n",
    "y eso en teoria podemos correrlo como pipieline, guardar esos archivos filtrados, y subir esos como tablas en big query."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTRO SUPER ESTRICTO DE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''\n",
    "    Function that cleans the text, using NLTK stopwords,\n",
    "    removes abnormal characters, converts everything to lowercase, and returns the preprocessed text\n",
    "    '''\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removal of invalid characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl_files(data_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            file_path = os.path.join(data_path, filename)\n",
    "            with jsonlines.open(file_path) as reader:\n",
    "                for review in reader:\n",
    "                    if 'text' in review and review['text']:\n",
    "                        processed_text = preprocess_text(review['text'])\n",
    "                        review['processed_text'] = processed_text\n",
    "                        data.append(review)\n",
    "    return data\n",
    "\n",
    "# Usage:\n",
    "data_path = 'data/data filtrada/Google Maps filtrado/Google Maps json/reviews-filtrados/'\n",
    "data = load_jsonl_files(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data count: 1116687\n",
      "Percentage of data lost: 29.841492828569866%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract the gmap_id from each object\n",
    "gmap_ids = [obj['gmap_id'] for obj in data]\n",
    "\n",
    "# Count the occurrences of each unique gmap_id\n",
    "gmap_id_counts = Counter(gmap_ids)\n",
    "\n",
    "# Filter the data based on the count condition\n",
    "filtered_data = [obj for obj in data if gmap_id_counts[obj['gmap_id']] >= 50]\n",
    "\n",
    "# Calculate the percentage of data lost\n",
    "data_lost_percentage = ((len(data) - len(filtered_data)) / len(data)) * 100\n",
    "\n",
    "# Print the filtered data count and percentage of data lost\n",
    "print(f\"Filtered data count: {len(filtered_data)}\")\n",
    "print(f\"Percentage of data lost: {data_lost_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original average rating: 4.294831883382349\n",
      "Filtered average rating: 4.277252264958758\n",
      "Data lost percentage: 29.841492828569866%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the gmap_id and rating from each object\n",
    "gmap_ids = [obj['gmap_id'] for obj in data]\n",
    "ratings = [obj['rating'] for obj in data]\n",
    "\n",
    "# Count the occurrences of each unique gmap_id\n",
    "gmap_id_counts = Counter(gmap_ids)\n",
    "\n",
    "# Filter the data based on the count condition\n",
    "filtered_data = [obj for obj in data if gmap_id_counts[obj['gmap_id']] >= 50]\n",
    "\n",
    "# Calculate the average rating for the original data\n",
    "original_avg_rating = sum(ratings) / len(ratings)\n",
    "\n",
    "# Calculate the average rating for the filtered data\n",
    "filtered_ratings = [obj['rating'] for obj in filtered_data]\n",
    "filtered_avg_rating = sum(filtered_ratings) / len(filtered_ratings)\n",
    "\n",
    "# Calculate the percentage of data lost\n",
    "data_lost_percentage = ((len(data) - len(filtered_data)) / len(data)) * 100\n",
    "\n",
    "# Compare the average ratings\n",
    "print(f\"Original average rating: {original_avg_rating}\")\n",
    "print(f\"Filtered average rating: {filtered_avg_rating}\")\n",
    "print(f\"Data lost percentage: {data_lost_percentage}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extract the gmap_id from each object\n",
    "gmap_ids = [obj['gmap_id'] for obj in data]\n",
    "\n",
    "# Count the occurrences of each unique gmap_id\n",
    "gmap_id_counts = Counter(gmap_ids)\n",
    "\n",
    "# Sort the counts in descending order\n",
    "sorted_counts = sorted(gmap_id_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted counts\n",
    "for gmap_id, count in sorted_counts:\n",
    "    print(f\"gmap_id: {gmap_id}, count: {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extract the gmap_id from each object\n",
    "gmap_ids = [obj['gmap_id'] for obj in data]\n",
    "\n",
    "# Count the occurrences of each unique gmap_id\n",
    "gmap_id_counts = Counter(gmap_ids)\n",
    "\n",
    "# Filter the data based on the count condition\n",
    "filtered_data = [obj for obj in data if gmap_id_counts[obj['gmap_id']] >= 50]\n",
    "\n",
    "# Print the filtered data count\n",
    "print(f\"Filtered data count: {len(filtered_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the output file path\n",
    "output_folder = r'data\\filtered_output'\n",
    "output_filename = 'uber-strict-filtered-reviews.jsonl'\n",
    "output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "# Save the filtered data to the output file\n",
    "with jsonlines.open(output_path, mode='w') as writer:\n",
    "    for review in filtered_data:\n",
    "        writer.write(review)\n",
    "\n",
    "# Print a success message\n",
    "print(f\"Filtered data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "# Read the first file\n",
    "file1_path = r'data\\data filtrada\\Google Maps filtrado\\Google Maps json\\google-strict-filtered-reviews.jsonl'\n",
    "df1 = pd.read_json(file1_path, lines=True)\n",
    "\n",
    "# Extract the unique 'gmap_id' values from the first file\n",
    "gmap_ids = df1['gmap_id'].unique()\n",
    "\n",
    "# Read the second file\n",
    "file2_path = r'data\\data filtrada\\Google Maps filtrado\\Google Maps json\\filtered_bars.json'\n",
    "filtered_data = []\n",
    "\n",
    "# Save the filtered data as strict-filtered-bars.jsonl with UTF-8 encoding\n",
    "output_path = r'data\\strict-filtered-bars2.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in filtered_data:\n",
    "        json.dump(item, file, ensure_ascii=False, check_circular=False)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(\"Filtering completed. Result saved as strict-filtered-bars.jsonl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gmap_ids.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
